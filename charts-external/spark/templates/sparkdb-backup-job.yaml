{{ if .Values.enabled }}{{ if .Values.dbBackupJob }}
#apiVersion: batch/v1
#kind: Job
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: sparkdb-backup
spec:
  schedule: {{ .Values.dbBackupSchedule | default "@daily" | quote }}
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          name: sparkdb-backup
        spec:
          containers:
          - name: ops
            image: orihoch/sk8s-ops:mysql
            resources:
              requests:
                cpu: "0.01"
                memory: "10Mi"
            command:
            - bash
            - "-c"
            - |
              source ~/.bashrc;
              EXPORT_URL="gs://{{ .Values.global.projectName }}-k8s-backups/sparkdb-{{ .Values.global.environmentName }}-dump-`date +%Y-%m-%d-%H-%M`.sql"
              echo "EXPORT_URL=${EXPORT_URL}"
              gcloud config set project "{{ .Values.global.googleProjectId }}" &&\
              mysqldump --host=sparkdb --port=3306 --protocol=tcp --user=root "--password=${MYSQL_ROOT_PASSWORD}" \
                        --all-databases > "dump.sql" &&\
              ls -lah dump.sql &&\
              gsutil cp dump.sql "${EXPORT_URL}" &&\
              echo "Great Success!"
            env:
            - name: MYSQL_ROOT_PASSWORD
              {{ if .Values.rootSecretName }}
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.rootSecretName | quote }}
                  key: MYSQL_ROOT_PASSWORD
              {{ else }}
              value: "123456"
              {{ end }}
            volumeMounts:
            - name: k8s-ops
              mountPath: /k8s-ops
              readOnly: true
          volumes:
          - name: k8s-ops
            secret:
              secretName: ops
          restartPolicy: Never
{{ end }}{{ end }}
